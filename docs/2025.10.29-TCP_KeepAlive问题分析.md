# TCP Keep-Alive 15秒问题分析与解决方案

## 问题描述

在 ThingsPanel-GMQTT 服务器上，观察到每隔 15 秒会自动发送 TCP Keep-Alive 探测包：
```
TCP Keep-Alive [1883->59768] [ACK] Seq=4 Ack=58 Win=64256 Len=0
```

经过测试确认：
- 在同一服务器上部署的普通 TCP Server **没有**这个 15 秒 Keep-Alive 现象
- 在另一台服务器部署 GMQTT 服务，**重现**了 15 秒 Keep-Alive 的现象
- **结论**: 这是 GMQTT（准确说是 Go 语言 net 包）的默认行为

## 根本原因

### 1. Go 语言的默认行为

从 **Go 1.8** 版本开始，Go 标准库 `net` 包在 `TCPListener.Accept()` 时会**自动启用 TCP Keep-Alive**，并且默认周期被硬编码为 **15 秒**。

**相关代码位置**（Go 标准库 `src/net/tcpsock.go`）:
```go
// Go 1.8+ 的默认行为
func (ln *TCPListener) accept() (*TCPConn, error) {
    fd, err := ln.fd.accept()
    if err != nil {
        return nil, err
    }
    tc := newTCPConn(fd)
    // 这里会自动设置 Keep-Alive
    if ln.lc.KeepAlive >= 0 {
        setKeepAlive(fd, true)
        if ln.lc.KeepAlive > 0 {
            setKeepAlivePeriod(fd, ln.lc.KeepAlive)
        }
    }
    return tc, nil
}

// 默认 Keep-Alive 周期是 15 秒（硬编码）
const defaultTCPKeepAlive = 15 * time.Second
```

### 2. GMQTT 的实现

GMQTT 在 `server/server.go:984` 的 `serveTCP()` 函数中使用标准的 `net.Listener.Accept()`：

```go
func (srv *server) serveTCP(l net.Listener) {
    defer func() {
        l.Close()
    }()
    var tempDelay time.Duration
    for {
        rw, e := l.Accept()  // <-- 这里接受连接，Go 会自动启用 Keep-Alive
        if e != nil {
            // 错误处理...
            return
        }
        if srv.hooks.OnAccept != nil {
            if !srv.hooks.OnAccept(context.Background(), rw) {
                rw.Close()
                continue
            }
        }
        client, err := srv.newClient(rw)
        if err != nil {
            zaplog.Error("new client fail", zap.Error(err))
            return
        }
        go client.serve()
    }
}
```

**问题**：GMQTT 没有显式禁用或配置 TCP Keep-Alive，因此使用了 Go 的默认值（15秒）。

### 3. 与 MQTT Keep-Alive 的区别

需要注意区分两种不同的 Keep-Alive：

| 类型 | 层级 | 默认值 | 配置位置 | 作用机制 |
|------|------|--------|----------|----------|
| **TCP Keep-Alive** | TCP 传输层 | 15秒（Go 默认） | `server/server.go` 中未配置 | 操作系统自动发送空 ACK 包探测连接 |
| **MQTT Keep-Alive** | MQTT 应用层 | 300秒 | `cmd/gmqttd/default_config.yml` 中 `max_keepalive: 300` | 客户端发送 PINGREQ，服务器回复 PINGRESP |

**重要说明**：

#### TCP Keep-Alive（传输层）
- **目的**: 检测死连接（如网络中断、对端崩溃、路由器故障）
- **机制**: 操作系统在 TCP 层自动发送空的 ACK 探测包
- **开销**: 每 15 秒发送一个空包（约 54 字节）
- **特点**:
  - 透明于应用层，应用程序不感知
  - 只能检测网络层面的连接问题
  - **无法检测应用层是否正常**（如进程死锁）

#### MQTT Keep-Alive（应用层）
- **目的**: 检测 MQTT 客户端是否存活，保持会话活跃
- **机制**:
  - 客户端必须在 Keep-Alive 周期内发送任何 MQTT 控制包（PUBLISH, SUBSCRIBE 等）
  - 如果没有其他消息，客户端发送 PINGREQ，服务器回复 PINGRESP
- **超时检测**: 服务器在 `1.5 * KeepAlive` 时间内未收到任何包，则断开连接（见 `server/client.go:404`）
- **特点**:
  - 应用层协议，确保 MQTT 协议栈正常工作
  - 能检测应用层问题（如客户端进程死锁、卡死）
  - **这是 MQTT 协议必需的功能**

#### 禁用 TCP Keep-Alive 是否影响 MQTT Keep-Alive？

**答案：完全不影响！两者独立工作。**

**原因**：
1. **MQTT Keep-Alive 是应用层协议的一部分**，由 MQTT 客户端和服务器实现，不依赖 TCP Keep-Alive
2. **代码实现位置**（`server/client.go:403-405`）：
   ```go
   if keepAlive := client.opts.KeepAlive; keepAlive != 0 { //KeepAlive
       _ = client.rwc.SetReadDeadline(time.Now().Add(time.Duration(keepAlive/2+keepAlive) * time.Second))
   }
   ```
   服务器通过设置 **读取超时**（1.5 倍 KeepAlive）来检测客户端，与 TCP Keep-Alive 无关

3. **客户端必须主动发送 PINGREQ**，这是 MQTT 协议要求，不受 TCP 层影响

#### 为什么可以安全禁用 TCP Keep-Alive？

| 场景 | TCP Keep-Alive 禁用后 | MQTT Keep-Alive 工作情况 |
|------|----------------------|------------------------|
| 客户端正常运行 | ✅ 无影响 | ✅ 客户端正常发送 PINGREQ/数据 |
| 客户端网络断开 | ⚠️ 无法立即检测 | ✅ 1.5 * KeepAlive 后超时断开 |
| 客户端进程崩溃 | ⚠️ 无法立即检测 | ✅ 1.5 * KeepAlive 后超时断开 |
| 中间路由器故障 | ⚠️ 无法立即检测 | ✅ 1.5 * KeepAlive 后超时断开 |
| 客户端应用卡死 | ❌ 无法检测（TCP 连接仍然正常） | ✅ 能检测到（无 PINGREQ） |

**结论**：
- TCP Keep-Alive 提供的是**更快的**网络层死连接检测（15秒 vs MQTT 的 300秒）
- 但对于 MQTT 应用来说，**MQTT Keep-Alive 已经足够**，可以在合理时间内（默认 450 秒）检测到所有问题
- 禁用 TCP Keep-Alive **不会影响 MQTT 协议的正常工作**

## 解决方案

### 选择建议

根据你的使用场景选择：

| 场景 | 推荐方案 | 原因 |
|------|---------|------|
| **生产环境，客户端网络稳定** | 禁用 TCP Keep-Alive | 减少网络开销，MQTT Keep-Alive 足够 |
| **客户端经常断网/移动网络** | 调整为 60-120 秒 | 更快检测网络断开，但不要太频繁 |
| **大量客户端连接** | 禁用 TCP Keep-Alive | 显著减少服务器和网络负担 |
| **需要快速检测死连接** | 调整为 30-60 秒 | 平衡检测速度和网络开销 |

**典型 IoT 场景推荐**：
- 如果你的 MQTT Keep-Alive 已经设置（如 60-300 秒），**建议禁用 TCP Keep-Alive**
- 如果客户端网络很不稳定，可以将 TCP Keep-Alive 设为 60-120 秒（不要 15 秒）

### 方案 1: 修改代码禁用 TCP Keep-Alive

修改 `server/server.go` 的 `serveTCP()` 函数，在接受连接后显式禁用或调整 Keep-Alive：

```go
func (srv *server) serveTCP(l net.Listener) {
    defer func() {
        l.Close()
    }()
    var tempDelay time.Duration
    for {
        rw, e := l.Accept()
        if e != nil {
            // 错误处理...
            return
        }

        // 新增：禁用或调整 TCP Keep-Alive
        if tcpConn, ok := rw.(*net.TCPConn); ok {
            // 选项 A: 完全禁用 TCP Keep-Alive
            tcpConn.SetKeepAlive(false)

            // 选项 B: 或者调整为更长的周期（如 2 小时）
            // tcpConn.SetKeepAlive(true)
            // tcpConn.SetKeepAlivePeriod(2 * time.Hour)
        }

        if srv.hooks.OnAccept != nil {
            if !srv.hooks.OnAccept(context.Background(), rw) {
                rw.Close()
                continue
            }
        }
        client, err := srv.newClient(rw)
        if err != nil {
            zaplog.Error("new client fail", zap.Error(err))
            return
        }
        go client.serve()
    }
}
```

**修改位置**: `server/server.go:990` 行之后

### 方案 2: 通过 OnAccept Hook 配置（推荐，无需修改核心代码）

利用 GMQTT 的 Hook 机制，在 `OnAccept` 钩子中配置 TCP Keep-Alive。

**修改位置**: `cmd/gmqttd/main.go` 或创建自定义插件

```go
func customOnAccept(ctx context.Context, conn net.Conn) bool {
    // 禁用或调整 TCP Keep-Alive
    if tcpConn, ok := conn.(*net.TCPConn); ok {
        // 禁用 TCP Keep-Alive
        tcpConn.SetKeepAlive(false)

        // 或者设置为更长的周期
        // tcpConn.SetKeepAlive(true)
        // tcpConn.SetKeepAlivePeriod(7200 * time.Second) // 2 小时
    }
    return true // 允许连接
}

// 在创建服务器时注册 Hook
hooks := server.Hooks{
    OnAccept: customOnAccept,
}
srv := server.New(
    server.WithHook(hooks),
    // ... 其他选项
)
```

### 方案 3: 系统级别调整（不推荐）

修改 Linux 系统的 TCP Keep-Alive 参数：

```bash
# 查看当前值
sysctl net.ipv4.tcp_keepalive_time
sysctl net.ipv4.tcp_keepalive_intvl
sysctl net.ipv4.tcp_keepalive_probes

# 临时修改（重启后失效）
sudo sysctl -w net.ipv4.tcp_keepalive_time=7200  # 改为 2 小时
sudo sysctl -w net.ipv4.tcp_keepalive_intvl=75
sudo sysctl -w net.ipv4.tcp_keepalive_probes=9

# 永久修改：编辑 /etc/sysctl.conf
net.ipv4.tcp_keepalive_time = 7200
net.ipv4.tcp_keepalive_intvl = 75
net.ipv4.tcp_keepalive_probes = 9
```

**注意**: 这会影响整个系统的所有 TCP 连接，不推荐使用。

## 推荐方案总结

**最佳方案**: **方案 2 - 通过 OnAccept Hook**

理由：
1. ✅ 不需要修改 GMQTT 核心代码
2. ✅ 易于维护和升级
3. ✅ 配置灵活，可以根据需求选择禁用或调整周期
4. ✅ 只影响 MQTT 服务，不影响系统其他服务

**具体实施步骤**:
1. 修改 `cmd/gmqttd/command/start.go` 文件
2. 在创建服务器时添加 `OnAccept` Hook
3. 在 Hook 中禁用 TCP Keep-Alive

## 代码修改示例

### 修改 `cmd/gmqttd/command/start.go`

在 `startCmd` 的 `Run` 函数中添加 Hook：

```go
// 找到创建服务器的位置，添加 Hook
hooks := server.Hooks{
    OnAccept: func(ctx context.Context, conn net.Conn) bool {
        if tcpConn, ok := conn.(*net.TCPConn); ok {
            // 禁用 TCP Keep-Alive
            _ = tcpConn.SetKeepAlive(false)
            // 或者设置为更长周期（如系统默认 2 小时）
            // _ = tcpConn.SetKeepAlive(true)
            // _ = tcpConn.SetKeepAlivePeriod(7200 * time.Second)
        }
        return true
    },
}

ln, err := net.Listen("tcp", cfg.Listeners[0].Address)
// ... 错误处理

srv := server.New(
    server.WithTCPListener(ln),
    server.WithConfig(cfg),
    server.WithHook(hooks), // 添加这一行
    // ... 其他选项
)
```

## 验证方法

修改后，使用 tcpdump 或 Wireshark 验证：

```bash
# 捕获 MQTT 端口的流量
sudo tcpdump -i any port 1883 -nn -tttt

# 观察是否还有 15 秒周期的 TCP Keep-Alive 包
# 如果禁用成功，将不会看到周期性的 [ACK] 包
```

## 参考资料

1. [Go net package - TCPConn.SetKeepAlive](https://pkg.go.dev/net#TCPConn.SetKeepAlive)
2. [Go net package - TCPConn.SetKeepAlivePeriod](https://pkg.go.dev/net#TCPConn.SetKeepAlivePeriod)
3. [GMQTT Hooks Documentation](server/hooks.go:100)
4. Linux TCP Keep-Alive: `man 7 tcp`

## 相关文件位置

- 核心连接处理: `server/server.go:984` (`serveTCP` 函数)
- Hook 定义: `server/hook.go:100` (`OnAccept` 钩子)
- 配置文件: `cmd/gmqttd/default_config.yml`
- 启动逻辑: `cmd/gmqttd/command/start.go`
